\section{Evaluation}
\label{sec:eval}

To experimentally evaluate the security and performance of temporal 
partitioning we simulated them in DRAMSim2\cite{DRAMSim2}, an open source 
simulator that models the memory controller and ranks, banks, and channels of 
the DRAM. To test the impact on the execution times of realistic benchmarks we 
integrated DRAMSim2 with the full architecture simulator, GEM5~\cite{gem5}. 

In all of the experiments in GEM5, we run the SPEC2006 benchmarks using the
ARM ISA. Each of our in-order cores uses the ``TimingSimple'' model in GEM5, 
and the out of order cores use the ``O3'' model. For each of the experiments, 
unless otherwise stated, we simulate two cores each running an independent 
SPEC2006 benchmark since many of the overheads of our schemes are present only 
when there are two or more isolation domains. The cache configuration used in 
most simulations (including sizes, associativity, and latencies) are derived 
from an the Intel Xeon E3-1220L which has two cores and is similar to the CPUs 
used in Amazon EC2 as of late 2013. Each of the two cores has 32kB L1i and L1d 
cache, a local 256kB L2 cache, and a shared 4MB L3 cache. DRAMSim2 is 
configured to simulate a 2GB DDR3 primary memory clocked at 667MHz. 
Our protection scheme requires a close-paged row buffer policy, though the 
baseline performs slightly better with an open-page policy. For this reason, 
the baseline uses an open-page policy in all experiments. 
Table~\ref{tab:config} summarizes the simulation infrastructure configurations, 
hough the configuration differs for certain experiments. 

For each experiment we fastforward through 1 billion instructions and simulate 
100 million instructions. All of the SPEC2006 benchmarks other than omnetpp and 
perlbench are used. Perlbench and omnetpp are omitted due to incompatibility 
with the infrastructure.

\begin{table}
    \caption{Configuration Parameters for the GEM5 \& DRAMSim2 simulator.}
    \begin{tabular}{|l|l|l|r|}
        \hline
        \multicolumn{3}{|l|}{In-Order Model}     & ``TimingSimple'' \\\hline
        \multicolumn{3}{|l|}{Out of Order Model} & ``O3''           \\\hline
        \multicolumn{3}{|l|}{Number of Cores}    & 2                \\\hline
        \hline
        \multicolumn{2}{|l|}{Memory}             & 2GB    & 667MHz  \\\hline
        \hline
        L1d / L1i          & 32kB   & 2-way  & 2 cycles\\\hline
        L2                 & 256kB  & 8-way  & 7 cycles \\\hline
        L3                 & 4MB    & 16-way & 17 cycles  \\\hline
    \end{tabular}
    \label{tab:config}
\end{table}

\subsection{Security Evaluation}
Temporal Partitioning eliminates the memory interference by modifying the 
queueing structure and scheduling algorithm of a memory controller. To prove 
the memory interference has indeed been eliminated, we run multi-program 
workloads from SPEC2006 benchmarks and record the timing of memory requests. We 
use GEM5 to collect memory request traces in 10 million instructions for each 
benchmark, then we run these traces in pairs of two (T0, T1) to monitor a 
two-core system in which each memory request trace is generated by a separate 
core and represents a different isolation domain. These traces are fed into 
DRAMSim2 to simulate the cycle-level behavior in the memory controller and DRAM 
device. 

To verify Temporal Partitioning can protect against timing channel attacks, we fix the 
benchmark in one isolation domain to T0 and run it with different benchmarks, T1. If the 
memory controller completely eliminates interference, the return time of each 
memory request in T0 should always be the same no matter what benchmark T1 is.  
We compared the results for a fixed T0 with different T1s.  
Figure~\ref{fig:Security_2} shows one example of the comparison. The Y 
axis is the return time difference for each memory request in T0 when we fix T0 
to be $bzip2$ and change T1 from $astar$ to $mcf$. We use two different turn 
lengths for TP, namely $T_w$ and $2^{12}$ memory clock cycles. As can be seen, 
both $TP-T_w$ and $TP-4096$ show a flat line that equals 0, meaning that the 
timing of T0's memory requests are not affected by which benchmark T1 is. On the 
contrary, FR-FCFS schemes show a huge difference after T1 changes from one 
benchmark to another, which proves the existence of memory interference and a
timing channel. We compared every possible combination of benchmarks in this 
way, and the results show that with Temporal Partitioning protection, the 
return time of every memory request from T0 stays the same no matter what 
benchmark T1 runs. 

\begin{figure}%[ht]
    \begin{center}
        \includegraphics[width=3.0in]{figs/Security_2.pdf}
        \caption{Memory Return Time Difference with Difference T1}
        \label{fig:Security_2}
        \vspace{-0.20in}
    \end{center}
\end{figure}

The timing channel protection feature is still 
maintained when extending to more than 2 isolation domains. To prove the scaling of 
isolation domains is secure, we run experiments with four traces. We fix T0 and 
change (T1, T2, T3) from ($astar$, $astar$, $astar$) to ($mcf$, $mcf$, $mcf$).  
We intentionally chose these two combinations because $astar$ is not 
memory-intensive while $mcf$ is very memory-intensive. The results for when T0 
is $bzip2$ are shown in Figure~\ref{fig:Security_4}. Similar to the results for 
two threads, the comparison passes for all combinations which proves TP eliminates the memory interference for multiple isolation domains. We ran this security evaluation for all benchmarks in SPEC2006, and the results show that there is no interference when using 
Temporal Partitioning scheme. 

\begin{figure}%[ht]
    \begin{center}
        \includegraphics[width=3.0in]{figs/Security_4.pdf}
        \caption{Return Time Difference with 4 Isolation Domains}
        \label{fig:Security_4}
        \vspace{-0.20in}
    \end{center}
\end{figure}

\subsection{Performance Evaluation}

\subsubsection{Row Buffer Policy}

Temporal partitioning cannot be easily implemented with an open page row buffer 
policy. Figure \ref{fig:rowbuff} examines the extent to which this is a drawback by comparing the execution times of the baseline controller with an 
close page policy and an open page policy. Figure 
\ref{fig:rowbuff} shows the percent difference in execution time 
of SPEC2006 benchmarks running with an open page policy and a close page 
policy over various L3 cache sizes %(the L1 and L2 configurations remain as 
%shown in Table \ref{tab:config}) 
and with in order and out of order cores. For 
positive values, the close page policy outperforms the open page policy. Even 
without an L3 cache, the difference is at most $0.04\%$ for the in order model and 
$0.15\%$ for out of order. The results suggest that the performance of the two
row buffer policies are comparable for these benchmarks.
%%% ED: sounds inconsistent to commented out for now
%For the remaining experiments, the close page policy will serve as 
%the baseline to highlight the other implications of temporal partitioning.

\begin{figure*}
    \begin{center}
        \subfloat[In Order]
        {
            \label{fig:open_io}
            \includegraphics[width=3.0in]{figs/open_io.eps}
        }
        \vspace{-0.10in}
        \subfloat[Out of Order]
        {
            \label{fig:open_o3}
            \includegraphics[width=3.0in]{figs/open_o3.eps}
        }
        \caption{Effect of Row Buffer Policy on Execution Time.}
        \label{fig:rowbuff}
    \end{center}
\end{figure*}

\subsubsection{Turn Length Selection}
\label{sec:turnlength}
There is a tradeoff in selecting the turn length for temporal partitioning. 
Shorter turns will have lower maximum memory latencies since the time a memory 
request must wait for other isolation domains to finish is shorter. Further, 
the overhead incurred whenever the active isolation domain has no requests to 
issue, but another does is reduced since the more intensive isolation domain 
can get back to issuing requests more quickly. For 
larger turns, the dead time is incurred less frequently and is better amortized 
over the execution time since the dead time is only entered at the end of each 
turn.

Figure \ref{fig:turnlength} shows the effect of turn length selection on 
execution time. For each benchmark, multiple execution times are obtained by
running that benchmark with another one, collecting results for all possible pairs.
Then, the execution times are normalized and averaged to be presented in the figure.
More specifically, the execution time, 
$T_{tp,b_{i}||b_{j}}$, for benchmark $b_i$ in isolation domain $i$ running 
with benchmark $b_{j}$ in isolation domain $j$ is normalized to 
$T_{base,b_{i}||b_{j}}$, the same execution time with the baseline. Then, 
they are averaged across all benchmarks run in isolation domain $j$,
\begin{equation}
    \label{eq:latavg}
    \sum_{j=1}^{B=10}\frac{T_{tp,b_{i}||b_{j}}}{T_{base,b_{i}||b_{j}}}\cdot\frac{1}{B},
\end{equation}
where B is 10, the number of benchmarks used. 
In the figure, $TP\_n$ 
indicates the timing protection using a turn length of 
$n$ and $TP\_WC$ indicates the minimum turn length, equal to the worst case 
time.

\begin{figure*}
    \begin{center}
        \subfloat[In Order]
        {
            \label{fig:tl_io.eps}
            \includegraphics[width=3.0in]{figs/tl_io.eps}
        }
        \vspace{-0.10in}
        \subfloat[Out of Order]
        {
            \label{fig:tl_o3.eps}
            \includegraphics[width=3.0in]{figs/tl_o3.eps}
        }
        \caption{Effect of Turn Length on excution Time.}
        \label{fig:turnlength}
    \end{center}
\end{figure*}

From the results it is clear that for these benchmarks a shorter turn length 
is preferable, however, a turn length of 64 outperforms the minimum turn length 
in most cases with the out of order core. With the minimum turn length, at most one request per turn can 
be issued. With a slightly higher turn length, multiple requests can be issued 
in each turn since not every request will require the worst case 
time to complete. For the in-order core this is less helpful since at most one 
request can be in-flight in the memory, however, the out of order core can take 
advantage of this with multiple in-flight memory requests. Since 64 appears to 
be the optimal point for these workloads, this turn length will be used in the 
remaining experiments.

\subsubsection{Execution Time Overhead and L3 Cache Sizes}

Figure \ref{fig:cache_sweep} shows the execution time overhead of temporal 
partitioning using the turn length of 64 and several L3 cache sizes. 
The execution overhead for each benchmark is reported in the same way as in Section
\ref{sec:turnlength}. Generally, the overheads are quite low with a 4MB 
cache. The execution time overhead for temporal partitioning is only $1.4\%$ on 
average using in-order cores, and $1.2\%$ using out-of-order cores. The 
benchmark, $astar$, incurs the least overhead because it has a small number of memory requests, 
while $mcf$ has the highest overhead of $4.8\%$ and $4.3\%$ for in-order and 
out-of-order cores respectively.

\begin{figure*}
    \begin{center}
        \subfloat[In Order]
        {
            \label{fig:tl_io.eps}
            \includegraphics[width=3.0in]{figs/cache_io.eps}
        }
        \vspace{-0.10in}
        \subfloat[Out of Order]
        {
            \label{fig:tl_o3.eps}
            \includegraphics[width=3.0in]{figs/cache_o3.eps}
        }
        \caption{Execution Time Overhead and the Effect of Cache Configuration.}
        \label{fig:cache_sweep}
    \end{center}
\end{figure*}

%The 4MB L3 cache is based on the Intel Xeon E3-1220L, a two-core varient of the 
%processors currently used in the Amazon EC2 cloud infrastructure, so this 
%configuration is most representitive of the performance overhead of the scheme. 
%However, 
Figure \ref{fig:cache_sweep} also shows the overhead with smaller 
L3 cache sizes to study the effect of the cache configuration on the execution 
time overhead. Each cache configuration used with temporal partitioning is 
compared against the baseline using the same cache configuration. Performance 
overhead for the out-of-order model increases for smaller cache configurations with a maximum overhead of $24\%$ for $mcf$ without an L3 cache or $13\%$ with an L3 cache of only 1 MB.


\subsubsection{Impact on Memory Latency}
\label{sec:latency}

The average memory latency can be used as a metric to measure memory 
performance, and the maximum memory latency has implications for quality of 
service (QoS) guarantees. 
%With temporal partitioning, the latency of 
%a request is the same as the FR-FCFS scheme if it is issued by the active isolation domain. 
%However, requests that enter the transaction queue of an 
%incactive isolation domain or are enqueued during the 
%dead time will be stalled until the next turn of its isolation demain.
We measure the time between one request entering the transaction queue in the
memory controller and 
returning to the core (or a cache) as the memory latency. 
This measurement is done for both isolation domains in the 2-core 
processor for each of the combinations of two benchmarks. We use different turn lengths to explore the impact of 
turn length on the memory latency. Figure \ref{fig:latency} shows the average and maximum latency for each
benchmark using in-order and out-of-order core models. From the maximum latency results, we can see the maximum
latency increases as the turn length increases. This is because the requests that enter the transaction queue of
an inactive isolation domain or are enqueued during the dead time will be stalled until the next turn of its isolation demain.
As a result, the time it waits in the transaction queue will be likely longer with a longer turn length. Therefore, for workloads with real-time constraints,
a short turn length is likely to be better to reduce the maximum latency. The average memory latency
also shows a similar trend and increases with the turn length.
The results suggest that an additional latency due to waiting for a turn usually outweighs
the overhead from dead time for most of the SPEC benchmarks.
However, there are exceptions when applications are more memory intensive and bandwidth-limited. 
For example, $hmmer$ running on an out-of-order core shows
a better average memory
latency with the turn length of 128 than 64. 

%the protection schemes on memory latency for several turn lengths. The mean memory latency during the 
%execution of 
%benchmark $b_i$ in isolation domain $i$ when run 
%alongside benchmark $b_j$ in isolation domain $j$ using temporal partitioning
%is $L_{tp,b_{i}||b_{j}}$. For $b_{i}$, we report the mean memory latency of a 
%benchmark execution averaged across each benchmark it is run in parallel with,
%\begin{equation}
%    \label{eq:latavg}
%    \sum_{j=1}^{I=10}{L_{tp,b_{i}||b_{j}}}\cdot\frac{1}{I},
%\end{equation}
%where I is the number of benchmarks used in these experiments (10). Similarly, 
%Figure \ref{fig:maxlatency} shows the maximum memory latencies during a program 
%execution averaged in the same way as the mean latency.

\begin{figure*}
    \begin{center}
        \subfloat[In Order Maximum]
        {
            \label{fig:tl_io.eps}
            \includegraphics[width=3.0in]{figs/maxlat_io.eps}
        }
        \vspace{-0.10in}
        \subfloat[Out of Order Maximum]
        {
            \label{fig:tl_o3.eps}
            \includegraphics[width=3.0in]{figs/maxlat_o3.eps}
        }
        \qquad
        \subfloat[In Order Mean]
        {
            \label{fig:tl_io.eps}
            \includegraphics[width=3.0in]{figs/avglat_io.eps}
        }
        \vspace{-0.10in}
        \subfloat[Out of Order Mean]
        {
            \label{fig:tl_o3.eps}
            \includegraphics[width=3.0in]{figs/avglat_o3.eps}
        }
        \caption{Effect of Turn Length on Memory Latency}
        \label{fig:latency}
    \end{center}
\end{figure*}

\subsubsection{Scalability}
As the number of isolation domains increases, the maximum latency for each 
isolation domain increases because requests belonging to the inactive isolation 
domain must wait for an increased number of turns for other domains to finish. This 
implies the overhead increases with the number of isolation domains. Figure 
\ref{fig:threadscale} shows the execution time overhead of temporal 
partitioning as the number of isolation domains increases. The average overhead is
shown for 
each benchmarks as before, except rather than running all combinations of 3 and 
4 isolation domains we restrict our analysis to a smaller number of combinations.
Isolation domains beyond the first use the same workload, that is, the same 
benchmark is repeated for isolation 
domain 2,3, and 4, but each benchmark is used.

\begin{figure*}
    \begin{center}
        \subfloat[In Order]
        {
            \label{fig:tl_io.eps}
            \includegraphics[width=3.0in]{figs/threadscale_io.eps}
        }
        \vspace{-0.10in}
        \subfloat[Out of Order]
        {
            \label{fig:tl_o3.eps}
            \includegraphics[width=3.0in]{figs/threadscale_o3.eps}
        }
        \caption{Effect of Scaling the Number of Isolation Domains}
        \label{fig:threadscale}
    \end{center}
\end{figure*}

%For the out-of-order core, $mcf$ with 4 isolation domains is omitted due to 
%difficulty collecting data. 
For each benchmark, the overhead increases with the 
number of isolation domains. For 3 isolation domains the average execution 
time overhead is $2.7\%$ and $1.9\%$ for the in-order and out-of-order models 
respectively. For 4 isolation domains this increases to $6.3\%$ for the in-order 
model. In all cases $mcf$ incurs the highest overhead as the number of other 
isolation domains increases since it is the most memory intensive benchmark. 
With more isolation domains, the average time that its memory requests are delayed 
is increased and computation cannot continue due to unreturned memory requests.

